def generate(model, tokenizer, prompt, max_new_tokens=30):  
    ids, _ = tokenizer.tokenize(prompt)

    for _ in range(max_new_tokens):
        context = ids{-3:}

          
