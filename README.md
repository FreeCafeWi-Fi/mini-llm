# mini-llm
My homemade LLM


# My 80M Homebrew LLM

Goal:
- Build a small, general-purpose transformer model (~80M parameters)
- Train it on a broad English dataset
- Make it a "jack of all trades" model
- Learn how LLMs work by building one from scratch

Planned components:
- Tokenizer
- Embedding layer
- Attention layers
- Feed-forward layers
- Training loop
- Sampling function
